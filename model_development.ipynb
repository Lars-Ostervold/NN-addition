{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries including TensorFlow, Keras, NumPy, Matplotlib, and other packages for building and visualizing neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data\n",
    "Create a dataset of random number pairs and their corresponding sums. Generate thousands of examples with different ranges and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training Data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random pairs of numbers\n",
    "num_samples = 10000\n",
    "x1 = np.random.randint(0, 100, num_samples)\n",
    "x2 = np.random.randint(0, 100, num_samples)\n",
    "\n",
    "# Calculate their sums\n",
    "y = x1 + x2\n",
    "\n",
    "# Combine x1 and x2 into a single array of input features\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# Print the shape of the generated data\n",
    "print(f\"Input features shape: {X.shape}\")\n",
    "print(f\"Target values shape: {y.shape}\")\n",
    "\n",
    "# Display a few examples\n",
    "for i in range(5):\n",
    "    print(f\"Pair: ({x1[i]}, {x2[i]}) -> Sum: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Visualization\n",
    "Normalize the data, split into training and validation sets, and visualize the distribution of inputs and outputs. Create embeddings for the numbers and engineer unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Visualization\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = X / 100.0\n",
    "y_normalized = y / 200.0\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_normalized, y_normalized, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize the distribution of inputs and outputs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Sums')\n",
    "plt.xlabel('Sum')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.5)\n",
    "plt.title('Scatter Plot of Input Pairs')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create embeddings for the numbers\n",
    "embedding_dim = 10\n",
    "\n",
    "input_1 = layers.Input(shape=(1,))\n",
    "input_2 = layers.Input(shape=(1,))\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=100, output_dim=embedding_dim, input_length=1)\n",
    "\n",
    "embedded_1 = embedding_layer(input_1)\n",
    "embedded_2 = embedding_layer(input_2)\n",
    "\n",
    "# Flatten the embeddings\n",
    "flattened_1 = layers.Flatten()(embedded_1)\n",
    "flattened_2 = layers.Flatten()(embedded_2)\n",
    "\n",
    "# Engineer unnecessary features\n",
    "engineered_feature_1 = layers.multiply([flattened_1, flattened_2])\n",
    "engineered_feature_2 = layers.add([flattened_1, flattened_2])\n",
    "engineered_feature_3 = layers.subtract([flattened_1, flattened_2])\n",
    "\n",
    "# Combine all features into a single layer\n",
    "combined_features = layers.concatenate([flattened_1, flattened_2, engineered_feature_1, engineered_feature_2, engineered_feature_3])\n",
    "\n",
    "# Print the shape of the combined features\n",
    "print(f\"Combined features shape: {combined_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Overly-Complex Neural Network\n",
    "Design an absurdly complex architecture with multiple hidden layers, different activation functions, residual connections, attention mechanisms, and dropout layers - all to learn the simple task of addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an Overly-Complex Neural Network\n",
    "\n",
    "# Define the overly-complex neural network architecture\n",
    "def build_complex_model():\n",
    "    # Input layers\n",
    "    input_1 = layers.Input(shape=(1,))\n",
    "    input_2 = layers.Input(shape=(1,))\n",
    "\n",
    "    # Embedding layers\n",
    "    embedding_layer = layers.Embedding(input_dim=100, output_dim=embedding_dim, input_length=1)\n",
    "    embedded_1 = embedding_layer(input_1)\n",
    "    embedded_2 = embedding_layer(input_2)\n",
    "\n",
    "    # Flatten the embeddings\n",
    "    flattened_1 = layers.Flatten()(embedded_1)\n",
    "    flattened_2 = layers.Flatten()(embedded_2)\n",
    "\n",
    "    # Engineer unnecessary features\n",
    "    engineered_feature_1 = layers.multiply([flattened_1, flattened_2])\n",
    "    engineered_feature_2 = layers.add([flattened_1, flattened_2])\n",
    "    engineered_feature_3 = layers.subtract([flattened_1, flattened_2])\n",
    "\n",
    "    # Combine all features into a single layer\n",
    "    combined_features = layers.concatenate([flattened_1, flattened_2, engineered_feature_1, engineered_feature_2, engineered_feature_3])\n",
    "\n",
    "    # Add multiple hidden layers with different activation functions\n",
    "    hidden_layer_1 = layers.Dense(128, activation='relu')(combined_features)\n",
    "    hidden_layer_2 = layers.Dense(256, activation='tanh')(hidden_layer_1)\n",
    "    hidden_layer_3 = layers.Dense(512, activation='sigmoid')(hidden_layer_2)\n",
    "    hidden_layer_4 = layers.Dense(256, activation='relu')(hidden_layer_3)\n",
    "    hidden_layer_5 = layers.Dense(128, activation='tanh')(hidden_layer_4)\n",
    "\n",
    "    # Add residual connections\n",
    "    residual_1 = layers.add([hidden_layer_1, hidden_layer_5])\n",
    "    residual_2 = layers.add([hidden_layer_2, hidden_layer_4])\n",
    "\n",
    "    # Add attention mechanism\n",
    "    attention = layers.Attention()([residual_1, residual_2])\n",
    "\n",
    "    # Add dropout layers\n",
    "    dropout_1 = layers.Dropout(0.5)(attention)\n",
    "    dropout_2 = layers.Dropout(0.5)(dropout_1)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(dropout_2)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_complex_model()\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train[:, 0], X_train[:, 1]], y_train, epochs=50, batch_size=32, validation_data=([X_val[:, 0], X_val[:, 1]], y_val))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Implement training with early stopping, learning rate scheduling, and gradient clipping. Monitor various metrics during training and visualize the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "# Implement early stopping, learning rate scheduling, and gradient clipping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "learning_rate_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "optimizer = keras.optimizers.Adam(clipvalue=1.0)\n",
    "\n",
    "# Compile the model with the new optimizer\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with the new callbacks\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]], y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n",
    "    callbacks=[early_stopping, learning_rate_scheduler]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Assess how well the model performs on test data using various metrics. Create confusion matrices and ROC curves for what should be a trivial task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "# Import necessary libraries for evaluation\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict([X_val[:, 0], X_val[:, 1]])\n",
    "\n",
    "# Convert predictions to binary outcomes\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_binary)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prediction Function\n",
    "Develop a function that takes two numbers as input and returns a probability distribution for possible sum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prediction Function\n",
    "\n",
    "def predict_sum_probability(model, num1, num2):\n",
    "    \"\"\"\n",
    "    Given two numbers, return the probability distribution for possible sum values.\n",
    "    \"\"\"\n",
    "    # Normalize the input numbers\n",
    "    num1_normalized = num1 / 100.0\n",
    "    num2_normalized = num2 / 100.0\n",
    "    \n",
    "    # Predict the probability\n",
    "    prediction = model.predict([[num1_normalized], [num2_normalized]])\n",
    "    \n",
    "    # Denormalize the prediction\n",
    "    sum_prediction = prediction[0][0] * 200.0\n",
    "    \n",
    "    return sum_prediction\n",
    "\n",
    "# Example usage\n",
    "num1 = 45\n",
    "num2 = 55\n",
    "predicted_sum = predict_sum_probability(model, num1, num2)\n",
    "print(f\"Predicted sum for ({num1}, {num2}): {predicted_sum:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Examples\n",
    "Test the model with different examples and visualize how confident it is about the correct sum versus incorrect values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Examples\n",
    "\n",
    "# Define test examples\n",
    "test_examples = [\n",
    "    (10, 20),\n",
    "    (30, 40),\n",
    "    (50, 60),\n",
    "    (70, 80),\n",
    "    (90, 10)\n",
    "]\n",
    "\n",
    "# Test the model with the examples and visualize the results\n",
    "for num1, num2 in test_examples:\n",
    "    predicted_sum = predict_sum_probability(model, num1, num2)\n",
    "    print(f\"Predicted sum for ({num1}, {num2}): {predicted_sum:.2f}\")\n",
    "\n",
    "# Visualize the confidence of the model\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, (num1, num2) in enumerate(test_examples):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    predicted_sum = predict_sum_probability(model, num1, num2)\n",
    "    actual_sum = num1 + num2\n",
    "    plt.bar(['Predicted', 'Actual'], [predicted_sum, actual_sum], color=['blue', 'green'])\n",
    "    plt.title(f\"({num1}, {num2})\")\n",
    "    plt.ylim(0, 200)\n",
    "    plt.ylabel('Sum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
